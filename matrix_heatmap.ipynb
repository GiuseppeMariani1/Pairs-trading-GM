{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9258b521"
      },
      "source": [
        "%pip install openpyxl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import yfinance as yf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time  # for rate-limit delays\n",
        "\n",
        "# Your full list of Yahoo-ready tickers (replace with your actual tickers)\n",
        "symbols = [\n",
        "    # Example tickers, replace with your full list\n",
        "    \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"FB\", \"BRK-B\", \"V\", \"JNJ\", \"WMT\",\n",
        "    # Add your tickers here...\n",
        "]\n",
        "\n",
        "# Date range for historical data\n",
        "start_date = \"2025-07-01\"\n",
        "end_date = \"2025-10-09\"\n",
        "\n",
        "def fetch_stock_data_batched(symbols, start_date, end_date, batch_size=100, delay=2):\n",
        "    all_data = []\n",
        "    for i in range(0, len(symbols), batch_size):\n",
        "        batch = symbols[i:i+batch_size]\n",
        "        print(f\"Downloading batch {i//batch_size + 1} / {int(np.ceil(len(symbols)/batch_size))}\")\n",
        "        try:\n",
        "            data = yf.download(batch, start=start_date, end=end_date, auto_adjust=True)[\"Close\"]\n",
        "            if isinstance(data.columns, pd.MultiIndex):\n",
        "                data.columns = data.columns.get_level_values(0)\n",
        "            all_data.append(data)\n",
        "            time.sleep(delay)\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading batch {i//batch_size + 1}: {e}\")\n",
        "    if all_data:\n",
        "        combined = pd.concat(all_data, axis=1)\n",
        "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
        "        combined = combined.dropna(axis=1, how='all')\n",
        "        return combined\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def calculate_daily_returns(stock_data):\n",
        "    return stock_data.pct_change(fill_method=None).dropna()\n",
        "\n",
        "def find_high_corr_pairs(correlation_matrix, threshold=0.85):\n",
        "    corr_upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "    high_corr = corr_upper.stack().reset_index()\n",
        "    high_corr.columns = ['Ticker1', 'Ticker2', 'Correlation']\n",
        "    high_corr = high_corr[high_corr['Correlation'] >= threshold]\n",
        "    return high_corr\n",
        "\n",
        "def plot_top_corr_cluster(correlation_matrix, high_corr_pairs, max_tickers=50):\n",
        "    top_tickers = pd.unique(high_corr_pairs[['Ticker1','Ticker2']].values.ravel())[:max_tickers]\n",
        "    sampled_corr = correlation_matrix.loc[top_tickers, top_tickers]\n",
        "\n",
        "    sns.clustermap(sampled_corr, cmap=\"coolwarm\", linewidths=0.3, figsize=(12,12))\n",
        "    plt.title(\"Top Correlated Cluster Heatmap\", y=1.05)\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    desired_count = 50\n",
        "    available_symbols = symbols.copy()\n",
        "    random.shuffle(available_symbols)\n",
        "\n",
        "    valid_tickers = []\n",
        "    batch_size = 25  # Smaller batches to check and add tickers iteratively\n",
        "\n",
        "    while len(valid_tickers) < desired_count and len(available_symbols) > 0:\n",
        "        # Pick next batch to try\n",
        "        next_batch_size = min(batch_size, desired_count - len(valid_tickers), len(available_symbols))\n",
        "        batch = [sym for sym in available_symbols[:next_batch_size] if sym not in valid_tickers]\n",
        "\n",
        "        if not batch:\n",
        "            break  # no new tickers left to try\n",
        "\n",
        "        print(f\"Trying batch of {len(batch)} tickers...\")\n",
        "        stock_data = fetch_stock_data_batched(batch, start_date, end_date)\n",
        "\n",
        "        # Identify tickers that downloaded successfully\n",
        "        successful = [sym for sym in batch if sym in stock_data.columns]\n",
        "\n",
        "        print(f\"Batch results: {len(successful)} successful tickers.\")\n",
        "\n",
        "        # Add successful tickers to valid list\n",
        "        valid_tickers.extend(successful)\n",
        "\n",
        "        # Remove all tickers in batch from available_symbols\n",
        "        available_symbols = [sym for sym in available_symbols if sym not in batch]\n",
        "\n",
        "        # Sleep between batches to avoid rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    if len(valid_tickers) == 0:\n",
        "        print(\"No valid tickers found. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Limit to exactly desired_count if more got added\n",
        "    valid_tickers = valid_tickers[:desired_count]\n",
        "\n",
        "    print(f\"Final list contains {len(valid_tickers)} tickers.\")\n",
        "\n",
        "    # Download full data for valid tickers to continue with analysis\n",
        "    stock_data = fetch_stock_data_batched(valid_tickers, start_date, end_date)\n",
        "\n",
        "    if stock_data.empty:\n",
        "        print(\"No data downloaded for valid tickers. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Calculate daily returns\n",
        "    daily_returns = calculate_daily_returns(stock_data)\n",
        "\n",
        "    # Compute correlation matrix\n",
        "    corr_matrix = daily_returns.corr()\n",
        "\n",
        "    # Find highly correlated pairs\n",
        "    high_corr_pairs = find_high_corr_pairs(corr_matrix, threshold=0.85)\n",
        "    print(f\"Found {len(high_corr_pairs)} highly correlated pairs (â‰¥0.85):\")\n",
        "    print(high_corr_pairs.head(20))\n",
        "\n",
        "    # Plot heatmap if there are correlated pairs\n",
        "    if len(high_corr_pairs) > 0:\n",
        "        print(\"Plotting heatmap of top correlated cluster...\")\n",
        "        plot_top_corr_cluster(corr_matrix, high_corr_pairs, max_tickers=50)\n",
        "    else:\n",
        "        print(\"No highly correlated pairs to plot.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "iAX1DmQxYijh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall problematic patsy and reinstall cleanly\n",
        "!pip uninstall -y patsy seaborn\n",
        "!pip install --no-cache-dir patsy seaborn"
      ],
      "metadata": {
        "id": "kBd6bcmJtnm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "Ky7xvy27vy-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XlAdm0SfLZfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Imports successful! Ready to go.\")"
      ],
      "metadata": {
        "id": "vt-2w2cruQID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir patsy seaborn matplotlib numpy pandas"
      ],
      "metadata": {
        "id": "s0HlGkNAulyd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}